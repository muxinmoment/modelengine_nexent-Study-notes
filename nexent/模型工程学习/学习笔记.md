# 神经网络：调整神经网络节点之间的连接权重，使得有学习功能
前向传播+反向传播，修改权重

# 典型的神经网络
卷积神经网络（CNN）-计算机视觉
循环神经网络（RNN）-自然语言处理

# transfomer出现
transformer对注意力机制进行了更复杂和可拓展的设计

# Token的概念被引入
Token可以是单词片段，小块图片，或者语音片段
transformer的目标是理解每个词上下文并预测下一个最可能出现的词
采用多头注意力机制进行大量并行计算，从多个角度理解不同的词之间的关系
注意力机制的输出本质是一个线性的输出，它融合了上下文重要的信息，还缺乏非线性表达能力
经过多次注意力机制和前馈神经网络后，机器通过对向量应用解嵌入矩阵并使用softmax推测下一个Tokem

# AI算力平台的组成
Scaling Law:模型表现随着计算量，数据量和参数量增加而提升
Emergent Abilities：大模型"涌现能力"普遍出现在进行10的22次方浮点运算（10B）之后
Chinchilla Law：Cbinchilla模型参数较少，训练数据多，其表现优于其他模型

# 大模型的发展在算力，显存，通信，存储带来巨大的压力，需要集群算力来满足要求
集群算力：算力基础设施

# 大模型对算力的需求=总计算量/时间要求

# 大模型对存力的要求（数据存储）
# 大模型对运力的要求（数通网络）
单个GPU早已无法满足大模型训练要求，多卡并行已经是大模型训练的必备技术

![alt text]({1A52A636-3AD2-43AC-BA79-511511AEF126}.png)

# AI发展趋势
![alt text](photo\{1A52A636-3AD2-43AC-BA79-511511AEF126}.png)
![alt text](photo\{16E8C9E8-C0D9-4FBC-ACDC-41B35D544478}.png)
![alt text](photo\{31234066-AACF-401A-81B1-9F1B91474135}.png)
![alt text](photo\{2BD01BF4-3B41-453F-BA7E-7479F96F6258}.png)

![alt text](photo\{813BBDF3-FB7A-48B9-A8B6-AFC0D729AF36}.png)
![alt text](photo\{651563A8-80EB-4F10-82EE-C325EBA1C4A1}.png)